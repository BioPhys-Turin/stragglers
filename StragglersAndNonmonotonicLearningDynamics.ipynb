{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Stragglers and non-monotonic learning dynamics in feed-forward neural\n",
        "networks**\n",
        "\n",
        "Simone Ciceri, Lorenzo Cassani, Matteo Osella, Pietro Rotondo, Filippo Valle, Marco Gherardi\n",
        "\n",
        "    Copyright (C) 2022 Marco Gherardi and Universit√† degli Studi di Milano\n",
        "\n",
        "    This program is free software: you can redistribute it and/or modify\n",
        "    it under the terms of the GNU General Public License as published by\n",
        "    the Free Software Foundation, either version 3 of the License, or\n",
        "    (at your option) any later version.\n",
        "\n",
        "    This program is distributed in the hope that it will be useful,\n",
        "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "    GNU General Public License for more details.\n",
        "\n",
        "    You should have received a copy of the GNU General Public License\n",
        "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "# INSTRUCTIONS\n",
        "\n",
        "Evaluate the SETUP cell first, then the DEMOs.\n",
        "\n",
        "To run the code on data sets different from those provided (MNIST, KMNIST, FashionMNIST, CIFAR10), change the code in the utility load_data (SETUP cell). The four data tensors (data, labels, test_data, test_labels) must contain the training set and test set. The shape of data and test_data is (number_of_elements, 1, X, Y), where, for instance, X=Y=28 for *MNIST. The shape of labels and test_labels is (number_of_elements). The variable input_size must be the size of each element of the data set. NOTE: the dataset used should be standardized, as is done in the SETUP cell."
      ],
      "metadata": {
        "id": "04wh12nOfrkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETUP"
      ],
      "metadata": {
        "id": "hkFAs0k5AI16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CviAmVyf9Imq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "######################\n",
        "# DATASET PARAMETERS #\n",
        "######################\n",
        "PDATA = 8192 # number of elements in the data set\n",
        "DATA_BLOCK = 1 # Data block to use within the full data set\n",
        "EPSILON = 0.000000001 # cutoff for the computation of the variance in the standardisation\n",
        "tdDATASET = torchvision.datasets.MNIST # the dataset (MNIST, KMNIST, FashionMNIST, CIFAR10)\n",
        "######################\n",
        "\n",
        "\n",
        "# Check if GPU is present and set device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(\"using GPU\")\n",
        "  !nvidia-smi\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"using CPU\")\n",
        "\n",
        "# Download the dataset\n",
        "dataset=tdDATASET(\"/content/\", train=True, download=True,\n",
        "                  transform = torchvision.transforms.ToTensor())\n",
        "\n",
        "# Standardize the data\n",
        "in_block = lambda n : (DATA_BLOCK-1)*PDATA <= n < DATA_BLOCK*PDATA\n",
        "data_means = torch.mean(torch.cat([a[0] for n,a in enumerate(dataset) if in_block(n)]), dim=0)\n",
        "data_vars = torch.sqrt(torch.var(torch.cat([a[0] for n,a in enumerate(dataset) if in_block(n)]), dim=0))\n",
        "transf = lambda x : (x - data_means)/(data_vars+EPSILON)\n",
        "\n",
        "# the training set\n",
        "dataset = tdDATASET(\"/content/\", train=True, download=True,\n",
        "                    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), transf]))\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=PDATA, shuffle=False)\n",
        "# the test set\n",
        "testset = tdDATASET(\"/content/\", train=False, download=True,\n",
        "                    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), transf]))\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=True)\n",
        "\n",
        "\n",
        "############ CLASSES ############\n",
        "\n",
        "# Base NN class\n",
        "# (computes the metric observables given a latent_representation, a.k.a. the activations of a hidden layer)\n",
        "class myNN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def latent_representation(self,X):\n",
        "    pass\n",
        "\n",
        "  # radii() computes both the radii and the distance between centers of mass\n",
        "  def radii(self, data, labels):\n",
        "    with torch.no_grad():\n",
        "      X = data[labels==0], data[labels==1]\n",
        "      nump = X[0].shape[0], X[1].shape[0]\n",
        "      X = self.latent_representation(X[0]), self.latent_representation(X[1])\n",
        "\n",
        "      # normalization\n",
        "      X = torch.nn.functional.normalize(X[0], dim=1), torch.nn.functional.normalize(X[1], dim=1)\n",
        "\n",
        "      # computation of the metric quantities\n",
        "      Xmean = torch.mean(X[0], dim=0), torch.mean(X[1], dim=0)\n",
        "      radius = ( torch.sqrt(torch.sum(torch.square(X[0]-Xmean[0]))/nump[0]) ,\n",
        "                 torch.sqrt(torch.sum(torch.square(X[1]-Xmean[1]))/nump[1]) )\n",
        "      distance = torch.norm(Xmean[0]-Xmean[1]).item()\n",
        "    return radius, distance\n",
        "\n",
        "\n",
        "# Derived class implementing a fully-connected NN\n",
        "# - in_size: size of input\n",
        "# - K: number of layers\n",
        "# - N_his: number of units in hidden layers (all equal)\n",
        "# - latent: ordinal number of hidden layer where the observables are computed\n",
        "class NN_KHL(myNN):\n",
        "  def __init__(self, in_size, K, N_hid, latent):\n",
        "    super().__init__()\n",
        "    self.latent = latent\n",
        "    self.layers = torch.nn.ModuleList([torch.nn.Linear(in_size, N_hid, bias=True)])\n",
        "    for _ in range(K-2):\n",
        "      self.layers.append(torch.nn.Linear(N_hid, N_hid, bias=True))\n",
        "    self.layers.append(torch.nn.Linear(N_hid, 2, bias=True))\n",
        "\n",
        "  def latent_representation(self, X):\n",
        "    X = X.view(-1,self.layers[0].in_features)\n",
        "    for l in range(self.latent):\n",
        "      X = self.layers[l](X)\n",
        "      X = torch.tanh(X)\n",
        "    return X\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.latent_representation(X)\n",
        "    for l in range(self.latent, len(self.layers)):\n",
        "      X = self.layers[l](X)\n",
        "      if l<len(self.layers)-1:\n",
        "        X = torch.tanh(X)\n",
        "    return X\n",
        "\n",
        "\n",
        "############ UTILITIES ############\n",
        "\n",
        "# Applies Gaussian noise to a tensor\n",
        "apply_noise = lambda noise,x: torch.normal(x,noise*torch.ones_like(x))\n",
        "\n",
        "\n",
        "# Returns data points and lables of the training and test set\n",
        "def load_data(data_block):\n",
        "  loader_it = iter(train_loader)\n",
        "\n",
        "  for _ in range(data_block):\n",
        "    data, labels = next(loader_it)\n",
        "  test_data, test_labels = next(iter(test_loader))\n",
        "\n",
        "  data, labels = data.to(device), labels.to(device)\n",
        "  test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
        "\n",
        "  # When using CIFAR10, convert to greyscale (1 channel)\n",
        "  if tdDATASET == torchvision.datasets.CIFAR10:\n",
        "    data = data[:,0,:,:]+data[:,1,:,:]+data[:,2,:,:]\n",
        "    test_data = test_data[:,0,:,:]+test_data[:,1,:,:]+test_data[:,2,:,:]\n",
        "\n",
        "  # Binarize class labels (NOTE: labels are 0,1 here but +1,-1 in the manuscript)\n",
        "  labels %= 2\n",
        "  test_labels %= 2\n",
        "\n",
        "  return data, labels, test_data, test_labels\n",
        "\n",
        "\n",
        "# Trains a model and returns errors, the metric quantities, misclassified examples at each epoch\n",
        "def train_and_measure(model, data, labels, test_data, test_labels, optimizer, criterion, epochs):\n",
        "  results_run = []\n",
        "  misclassified_examples_list = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(model(data), labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Compute errors and metric observables\n",
        "    train_error = torch.sum(torch.abs(torch.argmax(model(data),dim=1)-labels)).item()/data.shape[0]\n",
        "    test_error = torch.sum(torch.abs(torch.argmax(model(test_data),dim=1)-test_labels)).item()/len(testset)\n",
        "    radii, distance = model.radii(data, labels)\n",
        "    results_run.append([epoch, train_error, test_error, radii[0].item(), radii[1].item(), distance])\n",
        "    misclassified_examples = torch.argmax(model(data),dim=1)-labels != 0\n",
        "    misclassified_examples_list.append(misclassified_examples)\n",
        "\n",
        "  return results_run, misclassified_examples_list\n",
        "\n",
        "\n",
        "# Trains and stops when the inversion point is reached\n",
        "def train_stop_at_inversion(model, data, labels, optimizer, criterion):\n",
        "\n",
        "  radius, radius_prev = 0, 0\n",
        "  count = 0\n",
        "  radii = []\n",
        "\n",
        "  # This cycle trains until the inversion point is reached.\n",
        "  # The inversion point is reached when the first radius starts increasing.\n",
        "  # (does not halt during the initial 20 epochs to avoid being fooled by initial fluctuations)\n",
        "  while radius<radius_prev or count<20:\n",
        "    count += 1\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(model(data), labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    radius_prev = radius\n",
        "    (radius, _), _ = model.radii(data, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEMO"
      ],
      "metadata": {
        "id": "uq8bXAXttWAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Non monotonic dynamics of the metric quantities and invariance of the inversion point\n",
        "Computes the metric quantities as functions of the training error, throughout the training dynamics.\n",
        "\n",
        "**Expected output** - A plot showing the nonmonotonic dynamics of the metric quantities, where the inversion point is stable across different runs. (Training error on the x axis, manifold radii and inter-manifold distance on the y axis.) Similar results can be obtained for different architectures, by changing the parameters DEPTH and WIDTH.\n",
        "\n",
        "**Corresponding figures in the manuscript** - 1(c) and 1(d)\n",
        "\n",
        "**Expected run time** - On a Tesla T4, around 2 seconds per run (or ~20 seconds with N_RUNS=10 as below). On the CPU, around 10x longer. (These figures are for a 2-layer NN with 20 hidden units and 500 epochs.)"
      ],
      "metadata": {
        "id": "QHIUAMDbwxAO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui62OQl9FGNa"
      },
      "outputs": [],
      "source": [
        "#################################\n",
        "# MODEL AND TRAINING PARAMETERS #\n",
        "#################################\n",
        "DEPTH = 2\n",
        "WIDTH = 20\n",
        "LATENT = 1 # ordinal number of hidden layer where the observables are computed\n",
        "N_RUNS = 10 # number of runs from independent initializations\n",
        "EPOCHS = 500\n",
        "LEARNING_RATE = 0.1\n",
        "OPTIMIZER = torch.optim.SGD\n",
        "#################################\n",
        "\n",
        "# Load data\n",
        "data, labels, test_data, test_labels = load_data(DATA_BLOCK)\n",
        "input_size = data.shape[2]*data.shape[3] # 32*32 for CIFAR, 28*28 for *MNIST\n",
        "\n",
        "# Setup lists for results\n",
        "radii_data, distances_data, losses_data = [], [], []\n",
        "results = []\n",
        "\n",
        "# Perform N_RUNS independent training runs\n",
        "for niter in range(N_RUNS):\n",
        "  model = NN_KHL(input_size, DEPTH, WIDTH, LATENT).to(device)\n",
        "  optimizer = OPTIMIZER(model.parameters(), lr=LEARNING_RATE)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train\n",
        "  results_run, _ = train_and_measure(model, data, labels, test_data, test_labels, optimizer, criterion, EPOCHS)\n",
        "  results.append(results_run)\n",
        "\n",
        "# Plot results\n",
        "arr_res = np.array(results)\n",
        "for kk in range(len(arr_res)):\n",
        "  plt.plot(arr_res[kk,:,1],arr_res[kk,:,3], color=\"#3a5a4070\") # first radius VS training error\n",
        "  plt.plot(arr_res[kk,:,1],arr_res[kk,:,4], color=\"#67671570\") # second radius VS training error\n",
        "  plt.plot(arr_res[kk,:,1],arr_res[kk,:,5], color=\"#06467570\") # distance VS training error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Removal of stragglers from the training set eliminates the nonmonotonicity\n",
        "\n",
        "Prunes the training set by removing stragglers, defined as those elements of the training set that are still misclassified at the inversion point.\n",
        "\n",
        "**Expected output** - A plot showing that the dynamics on the pruned training set is monotonic: no inversion point exists. (Epochs on the x axis, manifold radii and inter-manifold distance on the y axis.)\n",
        "\n",
        "**Corresponding figures in the manuscript** - 2(a)\n",
        "\n",
        "**Expected run time** - On a Tesla T4, around 3.5 seconds per run (or ~35 seconds with N_RUNS=10 as below). On the CPU, around 10x longer. (These figures are for a 2-layer NN with 20 hidden units and 300 epochs.)"
      ],
      "metadata": {
        "id": "ANDeKvGot5pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# MODEL AND TRAINING PARAMETERS #\n",
        "#################################\n",
        "DEPTH = 2\n",
        "WIDTH = 20\n",
        "LATENT = 1 # ordinal number of hidden layer where the observables are computed\n",
        "N_RUNS = 10 # number of runs from independent initializations\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 0.1\n",
        "OPTIMIZER = torch.optim.SGD\n",
        "#################################\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "# Perform N_RUNS independent experiments\n",
        "for niter in range(N_RUNS):\n",
        "\n",
        "  # Load data\n",
        "  data, labels, test_data, test_labels = load_data(DATA_BLOCK)\n",
        "  input_size = data.shape[2]*data.shape[3] # 32*32 for CIFAR, 28*28 for *MNIST\n",
        "\n",
        "  # Use two identically initialized models, one for identifying stragglers, the other to train on the modified training set\n",
        "  # (NOTE: using differently initialized models gives almost identical results)\n",
        "  model = NN_KHL(input_size, DEPTH, WIDTH, LATENT).to(device)\n",
        "  model2 = NN_KHL(input_size, DEPTH, WIDTH, LATENT).to(device)\n",
        "  initial_state = model.state_dict()\n",
        "  model2.load_state_dict(initial_state)\n",
        "\n",
        "  optimizer = OPTIMIZER(model.parameters(), lr=LEARNING_RATE)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Do a single training run to identify the stragglers\n",
        "  train_stop_at_inversion(model, data, labels, optimizer, criterion)\n",
        "\n",
        "  # The stragglers are the misclassified data points now\n",
        "  stragglers = torch.argmax(model(data),dim=1)-labels != 0\n",
        "\n",
        "  # Prune the training set by removing the stragglers\n",
        "  data, labels = data[torch.logical_not(stragglers)], labels[torch.logical_not(stragglers)]\n",
        "\n",
        "  optimizer = OPTIMIZER(model2.parameters(), lr=LEARNING_RATE)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train again from scratch\n",
        "  results_run, _ = train_and_measure(model2, data, labels, test_data, test_labels, optimizer, criterion, EPOCHS)\n",
        "  results.append(results_run)\n",
        "\n",
        "\n",
        "# plot results\n",
        "arr_res = np.array(results)\n",
        "for kk in range(len(arr_res)):\n",
        "  plt.plot(arr_res[kk,:,0],arr_res[kk,:,3], color=\"#3a5a4070\") # first radius VS epochs\n",
        "  plt.plot(arr_res[kk,:,0],arr_res[kk,:,4], color=\"#67671570\") # second radius VS epochs\n",
        "  plt.plot(arr_res[kk,:,0],arr_res[kk,:,5], color=\"#06467570\") # distance VS epochs"
      ],
      "metadata": {
        "id": "Fbz4xSA5t9w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Removing stragglers from the training set affects the generalisation error\n",
        "\n",
        "Prunes the training set by removing the sets S(t), containing the misclassified examples at epoch t, and retrains.\n",
        "\n",
        "**Expected output** - A plot showing the test error (y axis) as a function of the pruned training set. On the x asis is the training error reached at the epoch t that defines S(t).\n",
        "\n",
        "**Corresponding figures in the manuscript** - 2(c)\n",
        "\n",
        "**Expected run time** - On a Tesla T4, around 6.5 minutes with N_RUNS=10 as below."
      ],
      "metadata": {
        "id": "R799kqcnj8Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# MODEL AND TRAINING PARAMETERS #\n",
        "#################################\n",
        "DEPTH = 2\n",
        "WIDTH = 20\n",
        "LATENT = 1 # ordinal number of hidden layer where the observables are computed\n",
        "N_RUNS = 10 # number of runs from independent initializations\n",
        "EPOCHS = 300 # maximum number of epochs reached in the identification of misclassified examples\n",
        "LOG_EPOCH_SKIP = 0.1 # logarithmic spacing between identifications of misclassified examples used for measuring the test error\n",
        "TEST_EPOCHS = 500 # number of epochs to measure the test error\n",
        "NOISE_SIGMA = 1. # standard deviation of the noise added to the test set\n",
        "LEARNING_RATE = 0.2\n",
        "OPTIMIZER = torch.optim.SGD\n",
        "#################################\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "# Perform N_RUNS independent experiments\n",
        "for niter in range(N_RUNS):\n",
        "\n",
        "  # Load data\n",
        "  dataALL, labelsALL, test_data, test_labels = load_data(DATA_BLOCK)\n",
        "  test_data = apply_noise(NOISE_SIGMA, test_data)\n",
        "  input_size = dataALL.shape[2]*dataALL.shape[3] # 32*32 for CIFAR, 28*28 for *MNIST\n",
        "\n",
        "  # Use two identically initialized models, one for identifying stragglers, the other to train on the modified training set\n",
        "  # (NOTE: using differently initialized models gives almost identical results)\n",
        "  model = NN_KHL(input_size, DEPTH, WIDTH, LATENT).to(device)\n",
        "  model2 = NN_KHL(input_size, DEPTH, WIDTH, LATENT).to(device)\n",
        "  initial_state = model.state_dict()\n",
        "  model2.load_state_dict(initial_state)\n",
        "\n",
        "  # Identify the misclassified examples at each epoch\n",
        "  optimizer = OPTIMIZER(model2.parameters(), lr=LEARNING_RATE)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  results_identification_run, misclassified_examples = train_and_measure(model2, dataALL, labelsALL, test_data, test_labels, optimizer, criterion, EPOCHS)\n",
        "\n",
        "  results_test_epoch = []\n",
        "\n",
        "  # Cycle over the (logarithmically spaced) epochs defining the pruned training sets\n",
        "  last_log_epoch = 0.\n",
        "  for epoch, mask in enumerate(misclassified_examples):\n",
        "\n",
        "    if np.log(epoch+1)-last_log_epoch > LOG_EPOCH_SKIP:\n",
        "      last_log_epoch = np.log(epoch+1)\n",
        "\n",
        "      # Prune the training set by removing the stragglers\n",
        "      data, labels = dataALL[torch.logical_not(mask)], labelsALL[torch.logical_not(mask)]\n",
        "\n",
        "      # restore initial state\n",
        "      model2.load_state_dict(initial_state)\n",
        "\n",
        "      optimizer = OPTIMIZER(model2.parameters(), lr=LEARNING_RATE)\n",
        "      criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "      # Train from scratch\n",
        "      results_run, _ = train_and_measure(model2, data, labels, test_data, test_labels, optimizer, criterion, TEST_EPOCHS)\n",
        "      results_test_epoch.append( (epoch, results_identification_run[epoch][1], results_run[-1][2]) )\n",
        "\n",
        "  results.append(results_test_epoch)\n",
        "\n",
        "  # Plot all results up to this run\n",
        "  for results_run in results:\n",
        "    plt.plot(np.array(results_run)[:,1], np.array(results_run)[:,2])\n",
        "    display.clear_output()\n",
        "    display.display(plt.gcf())\n",
        "\n",
        "# Plot all results\n",
        "display.clear_output()\n",
        "for results_run in results:\n",
        "  plt.plot(np.array(results_run)[:,1], np.array(results_run)[:,2])"
      ],
      "metadata": {
        "id": "fb_VQP_wnECJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Stragglers are the most stable set of misclassified points at any given epoch\n",
        "\n",
        "Computes the z-score, measuring the stability, between two independent runs, of the misclassified points evaluated at a given epoch t. For values of t in a specified range, performs N_RUNS iterations, training each time from scratch. The z-score is computed by averaging over the iterations.\n",
        "\n",
        "**Expected output** - A plot showing the z-score (y axis) as a function of the (average) training error (x axis) reached at epoch t.\n",
        "\n",
        "**Corresponding figures in the manuscript** - 2(d)\n",
        "\n",
        "**Expected run time** - On a Tesla T4, around 40 minutes with parameters as below"
      ],
      "metadata": {
        "id": "bcWAtLtXuCqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# MODEL AND TRAINING PARAMETERS #\n",
        "#################################\n",
        "DEPTH = 2\n",
        "WIDTH = 20\n",
        "LATENT = 1 # ordinal number of hidden layer where the observables are computed\n",
        "N_RUNS = 500 # number of runs from independent initializations\n",
        "MIN_EPOCH = 5 # lowest epoch in the cycle over the epochs defining the misclassified examples\n",
        "MAX_EPOCH = 70 # largest epoch\n",
        "SKIP_EPOCH = 5 # increment\n",
        "LEARNING_RATE = 0.2\n",
        "OPTIMIZER = torch.optim.SGD\n",
        "MAX_COMMON = 200 # maximum number of common examples considered in the hypergeometric model\n",
        "#################################\n",
        "\n",
        "\n",
        "# To compute the hypergeometric PDF\n",
        "from scipy.stats import hypergeom\n",
        "\n",
        "\n",
        "# Load data\n",
        "data, labels, test_data, test_labels = load_data(DATA_BLOCK)\n",
        "#test_data = apply_noise(NOISE_SIGMA, test_data)\n",
        "input_size = data.shape[2]*data.shape[3] # 32*32 for CIFAR, 28*28 for *MNIST\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "# Cycle over the epoch at which misclassified points are evaluated\n",
        "for misclassified_epoch in range(MIN_EPOCH, MAX_EPOCH, SKIP_EPOCH):\n",
        "\n",
        "  # Will contain the number of common instances in the two realizations\n",
        "  number_common = []\n",
        "\n",
        "  # To measure the number of common instances in the hypergeometric model\n",
        "  x = np.arange(0,MAX_COMMON)\n",
        "  random_common = np.zeros(x.shape) # Will contain the probability distribution function\n",
        "\n",
        "  training_errors = []\n",
        "\n",
        "  # Cycle on N_RUNS independent iterations\n",
        "  for niter in range(N_RUNS):\n",
        "\n",
        "    misclassified_examples = []\n",
        "    training_errors = []\n",
        "\n",
        "    # Two independent realizations\n",
        "    for _ in \"two\", \"realizations\":\n",
        "\n",
        "      model = NN_KHL(input_size, DEPTH, WIDTH, LATENT).to(device)\n",
        "      optimizer = OPTIMIZER(model.parameters(), lr=LEARNING_RATE)\n",
        "      criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "      # Train for misclassified_epoch epochs\n",
        "      results_realization, misclassified_examples_realization = train_and_measure(model, data, labels, test_data, test_labels, optimizer, criterion, misclassified_epoch)\n",
        "\n",
        "      # The training errors and the list of misclassified examples for the 2 realizations\n",
        "      training_errors.append(results_realization[-1][1])\n",
        "      misclassified_examples.append(misclassified_examples_realization[-1])\n",
        "\n",
        "    # Measure the number of common instances between misclassified examples and compute the PDF in the hypergeometric model\n",
        "    number_common.append(torch.sum(misclassified_examples[0]*misclassified_examples[1]).item())\n",
        "    random_common += hypergeom(PDATA, torch.sum(misclassified_examples[0]).cpu().numpy(), torch.sum(misclassified_examples[1]).cpu().numpy()).pmf(x)\n",
        "\n",
        "  zscore = (np.mean(number_common)-np.average(x, weights=random_common))/np.std(number_common)\n",
        "  results.append( [np.mean(training_errors), zscore] )\n",
        "\n",
        "# Plot the results\n",
        "data = np.array(results)\n",
        "plt.scatter(data[:,0], data[:,1])"
      ],
      "metadata": {
        "id": "XFBrgfzYuEG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "hkFAs0k5AI16",
        "QHIUAMDbwxAO",
        "ANDeKvGot5pK",
        "R799kqcnj8Ip",
        "bcWAtLtXuCqe"
      ],
      "authorship_tag": "ABX9TyNLOchz0Hv67J08cFaRhKf+"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}